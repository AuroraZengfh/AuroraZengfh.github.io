<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Publications - Fanhu Zeng</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>

<!-- Navigation Bar -->
<nav style="width:100%; background-color:#f8f8f8; padding:10px 0; border-bottom:1px solid #ddd;">
  <div style="max-width:800px; margin:0 auto; display:flex; justify-content:flex-end; gap:20px;">
    <a href="index.html" style="text-decoration:none; font-weight:bold; color:#333;">Home</a>
    <a href="publications.html" style="text-decoration:none; font-weight:bold; color:#333;">Publications</a>
    <!-- <a href="cv.html" style="text-decoration:none; font-weight:bold; color:#333;">CV</a> -->
  </div>
</nav>

<!-- Page Content -->
<div style="max-width:800px; margin:30px auto; padding:0 20px;">
  <h1>Publications</h1>
  <p>* indicates equal contribution</p>
  <!-- <b style="font-size: 30px">2025</b> -->


          <table style="width:100%;border:0px;border-spacing:0px 30px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top: -10px;"><tbody>
          <!--/ Publications -->

			<tr>
            <td style="margin-top:8px;font-size: 30px">
               <b>2026</b>
            </td>
            </tr>
			  
   	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/AAAI26_TRDQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>TR-DQ: Time-Rotation Diffusion Quantization</u></span>
			<div style="margin-top:8px;">
               Yihua Shao, Deyang Lin, Minxi Yan, Siyu Chen, <strong>Fanhu Zeng</strong>, Minwen Liao, Ao Ma, Ziyang Yan, Haozhe Wang, Yan Wang, Zhi Chen, Xiaofeng Cao, Haotong Qin, Hao Tang, Jingcai Guo 
	      </div>
				<div style="margin-top:8px;">
				<em>The 40th Annual AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2026
			  </div> 
			<div style="margin-top:8px;">
              <a href="https://arxiv.org/abs/2503.06564">arXiv</a>
              </div>
            </td>
          </tr>
			  
          <tr>
            <td style="margin-top:8px;font-size: 30px">
               <b>2025</b>
            </td>
            </tr>
          
	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS25_RobustMerge.png' alt="dise">
            </td>
            <td width="75%" valign="center" style="line-height:1.6;">
              <span class="papertitle"><u>RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness</u></span>
              <div style="margin-top:8px;">
               <strong>Fanhu Zeng</strong>, Haiyang Guo, Fei Zhu, Li Shen, Hao Tang
              </div> 
               <div style="margin-top:8px;">
              <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2025, <span style="color:red"><strong>Spotlight, acceptance rate: 3.1%</strong> </span>
              </div> 
              <div style="margin-top:8px;">
              <a href="https://arxiv.org/abs/2502.17159">arXiv</a> / <a href="https://github.com/AuroraZengfh/RobustMerge">Code</a>
              </div> 
            </td>
          </tr>
	  
	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/EMNLP25_ModalPrompt.png' alt="dise">
            </td>
            <td width="75%" valign="center" style="line-height:1.6;">
              <span class="papertitle"><u>ModalPrompt: Towards Efficient Multimodal Continual Instruction Tuning with Dual-Modality Guided Prompt</u></span>
              <div style="margin-top:8px;">
               <strong>Fanhu Zeng</strong>, Fei Zhu, Haiyang Guo, Xu-Yao Zhang, Cheng-Lin Liu
              </div> 
               <div style="margin-top:8px;">
              <em>The 2025 Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>)</em>, 2025
              </div> 
              <div style="margin-top:8px;">
              <a href="https://aclanthology.org/2025.emnlp-main.609">Paper</a> / <a href="https://arxiv.org/abs/2410.05849">arXiv</a> / <a href="https://github.com/AuroraZengfh/ModalPrompt">Code</a>
              </div> 
            </td>
          </tr>

	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICCV25_FCIT.png' alt="dise">
            </td>
            <td width="75%" valign="center" style="line-height:1.6;">
              <span class="papertitle"><u>Federated Continual Instruction Tuning</u></span>
              <div style="margin-top:8px;">
               Haiyang Guo, <strong>Fanhu Zeng</strong>, Fei Zhu, Wenzhuo Liu, Da-Han Wang, Jian Xu, Xu-Yao Zhang, Cheng-Lin Liu
              </div> 
               <div style="margin-top:8px;">
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
              </div> 
              <div style="margin-top:8px;">
              <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Federated_Continual_Instruction_Tuning_ICCV_2025_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2503.12897">arXiv</a> / <a href="https://github.com/Ghy0501/FCIT">Code</a>
              </div> 
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICCVW25_MCITlib.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</u></span>
              <div style="margin-top:8px;">
              Haiyang Guo, Fei Zhu, Hongbo Zhao, <strong>Fanhu Zeng</strong>, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang
              </div> 
              <div style="margin-top:8px;">
              <em> Workshop on Multimodal Continual Learning, International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025, <span style="color:red"><strong>Oral</strong> </span>
              </div> 
              <div style="margin-top:8px;">
              <a href="https://arxiv.org/abs/2508.07307">arXiv</a> / <a href="https://github.com/Ghy0501/MCITlib">Code</a>
              </div> 
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MM25_EventVAD.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>EventVAD: Training-Free Event-Aware Video Anomaly Detection</u></span>
              <div style="margin-top:8px;">
              Yihua Shao, Haojin He, Sijie Li, Siyu Chen, Xinwei Long, <strong>Fanhu Zeng</strong>, Yuxuan Fan, Muyang Zhang, Ziyang Yan, Ao Ma, Xiaochen Wang, Hao Tang, Yan Wang, Shuyan Li
              </div> 
              <div style="margin-top:8px;">
              <em> The 33rd ACM International Conference on Multimedia (<strong>ACM MM</strong>)</em>, 2025, <span style="color:red"><strong>Oral</strong> </span>
              </div> 
              <div style="margin-top:8px;">
              <a href="https://dl.acm.org/doi/10.1145/3746027.3754500">Paper</a> / <a href="https://arxiv.org/abs/2504.13092">arXiv</a> / <a href="https://github.com/YihuaJerry/EventVAD">Code</a>
              </div> 
            </td>
          </tr>

	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ACL25_HiDe-LLaVA.png' alt="dise">
            </td>
            <td width="75%" valign="center" style="line-height:1.6;">
              <span class="papertitle"><u>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model</u></span>
              <div style="margin-top:8px;">
               Haiyang Guo*, <strong>Fanhu Zeng*</strong>, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
              </div> 
               <div style="margin-top:8px;">
              <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2025
              </div> 
              <div style="margin-top:8px;">
              <a href="https://aclanthology.org/2025.acl-long.666">Paper</a> / <a href="https://arxiv.org/abs/2503.12941">arXiv</a> / <a href="https://github.com/Ghy0501/HiDe-LLaVA">Code</a>
              </div> 
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ACL25_Chartedit.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMsâ€™ Capability via Chart Editing</u></span>
              <div style="margin-top:8px;">
	           Xuanle Zhao*, Xuexin Liu*, Haoyue Yang*, Xianzhen Luo, <strong>Fanhu Zeng</strong>, Jianling Li, Qi Shi, Chi Chen
           </div>   
           <div style="margin-top:8px;">
              <em>Findings of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2025
              </div>
              <div style="margin-top:8px;">
              <a href="https://aclanthology.org/2025.findings-acl.185">Paper</a> / <a href="https://arxiv.org/abs/2505.11935">arXiv</a> / <a href="https://github.com/xxlllz/ChartEdit">Code</a>
              </div>
            </td>
          </tr>

            
	 <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR25_MambaIC.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>MambaIC: State Space Models for High-Performance Learned Image Compression</u></span>
              <div style="margin-top:8px;">
              <strong>Fanhu Zeng</strong>, Hao Tang, Yihua Shao, Siyu Chen, Ling Shao, Yan Wang
              </div> 
              <div style="margin-top:8px;">
              <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              </div> 
              <div style="margin-top:8px;">
              <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_MambaIC_State_Space_Models_for_High-Performance_Learned_Image_Compression_CVPR_2025_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2503.12461">arXiv</a> / <a href="https://github.com/AuroraZengfh/MambaIC">Code</a>
              </div> 
            </td>
          </tr>
		  
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICLR25_Local-Prompt.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection</u></span>
              <div style="margin-top:8px;">
              <strong>Fanhu Zeng</strong>, Zhen Cheng, Fei Zhu, Hongxin Wei, Xu-Yao Zhang
              </div> 
              <div style="margin-top:8px;">
              <em>The Thirteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              </div> 
              <div style="margin-top:8px;">
              <a href="https://openreview.net/forum?id=Ew3VifXaxZ">Paper</a> / <a href="https://arxiv.org/abs/2409.04796">arXiv</a> / <a href="https://github.com/AuroraZengfh/Local-Prompt">Code</a>
              </div> 
            </td>
          </tr>

          <tr>
            <td style="margin-top:8px;font-size: 30px">
               <b>2024</b>
            </td>
            </tr>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NIPSW24_M2MTAG.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle"><u>M2M-TAG: Training-Free Many-to-Many Token Aggregation for Vision Transformer Acceleration</u></span>
              <div style="margin-top:8px;">
               <strong>Fanhu Zeng</strong>, Deli Yu
              </div>
               <div style="margin-top:8px;">
	     <em>Workshop on Machine Learning and Compression, Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
	     </div> 
         <div style="margin-top:8px;">
              <a href="https://openreview.net/forum?id=LO3Mw8Jrk0">Paper</a>
              </div>
            </td>
          </tr>

        </tbody></table>
</div>

</body>
</html>
